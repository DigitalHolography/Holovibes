#pragma once

/*! This namespace contains internal private functions.
 * They must not be called from outisde the object.
 *
 * As the templated functions needs to be implemented in .cuhxx,
 * the internal private functions must also be implemented in .cuhxx.
 * However, by doing so, those functions are exported.
 */

namespace _private
{
/*! \brief Basic kernel to apply a map operation.
 *
 * Apply func on every pixel of the input and store
 * it in output.
 */
template <typename I, typename O, typename FUNC>
__global__ static void kernel_map_generic(const I* const input, O* const output, const size_t size, const FUNC func)
{
    const size_t index = threadIdx.x + blockIdx.x * blockDim.x;

    if (index < size)
    {
        output[index] = func(input[index]);
    }
}

/*! \brief Wrapper to call kernel map generic
 *
 * This function lauches kernel calls in the most optimized way.
 */
template <typename I, typename O, typename FUNC>
static void
_map_regular(const I* const input, O* const output, const size_t size, const FUNC func, const cudaStream_t stream)
{
    // Using 64 threads per blocks is more efficient than using the max number
    // of threads supported
    constexpr uint threads = 64;
    const uint blocks = map_blocks_to_problem(size, threads);

    kernel_map_generic<<<blocks, threads, 0, stream>>>(input, output, size, func);
    cudaCheckError();
}

/*! \brief Vectorize version of map operation
 *
 * This operation only works with cuda vector types (float4, ushort4...).
 * Thus, the size must be divisble by 4.
 *
 * If users wish to map an array of custom types. This can be done by creating
 * a structure of 4 element.
 * (i.e class myClass comes the vectorize type myClass4 { myClass x; myClass y; myClass z; myClass w };)
 *
 *
 * This function is non static because the created lambda has a closure containing
 * the func parameter. This forbids from making this function static.
 *
 */
template <typename T, typename FUNC>
void _map_vectorize(
    const T* const input, T* const output, const size_t size, const FUNC func, const cudaStream_t stream)
{
    CHECK(size % 4 == 0, "Map vectorize requires a size divisible by 4.");

    // Instead of one thread per pixel, in vectorize map each thread works on 4 pixels.
    // The 4 pixels are grouped in the vectorize type (T)
    const auto vectorized_lambda = [func] __device__(const T in) -> T
    { return T{func(in.x), func(in.y), func(in.z), func(in.w)}; };

    // Call regular generic map with the new vectorized lambda and an updated size
    _map_regular(input, output, size / 4, vectorized_lambda, stream);
}
} // namespace _private

/***** Generic map implementations *****/

template <typename I, typename O, typename FUNC>
void map_generic(const I* const input, O* const output, const size_t size, const FUNC func, const cudaStream_t stream)
{
    _private::_map_regular(input, output, size, func, stream);
}

template <typename FUNC>
void map_generic(
    const float* const input, float* const output, const size_t size, const FUNC func, const cudaStream_t stream)
{
    if (size % 4 == 0)
    {
        _private::_map_vectorize<float4>(reinterpret_cast<const float4* const>(input),
                                         reinterpret_cast<float4* const>(output),
                                         size,
                                         func,
                                         stream);
    }
    else
    {
        _private::_map_regular(input, output, size, func, stream);
    }
}

template <typename FUNC>
void map_generic(
    const ushort* const input, ushort* const output, const size_t size, const FUNC func, const cudaStream_t stream)
{
    if (size % 4 == 0)
    {
        _private::_map_vectorize<ushort4>(reinterpret_cast<const ushort4* const>(input),
                                          reinterpret_cast<ushort4* const>(output),
                                          size,
                                          func,
                                          stream);
    }
    else
        _private::_map_regular(input, output, size, func, stream);
}

template <typename T>
void map_divide(const T* const input, T* const output, const size_t size, const T value, const cudaStream_t stream)
{
    const auto divide = [value] __device__(const T input_pixel) -> T { return input_pixel / value; };

    map_generic(input, output, size, divide, stream);
}

template <typename T>
void map_multiply(const T* const input, T* const output, const size_t size, const T value, const cudaStream_t stream)
{
    const auto multiply = [value] __device__(const T input_pixel) -> T { return input_pixel * value; };

    map_generic(input, output, size, multiply, stream);
}
