#pragma once

#include "cuda.h"
#include "cuda_runtime.h"

#include <cassert>
#include <limits>

#include "common.cuh"
#include "cuda_memory.cuh"

/*! Mask used by the warp reduce using registers.
 * This mask means every thread should be processed
 */
#define FULL_MASK 0xffffffff
/*! \brief Number of threads handle by an unique warp */
static constexpr uint warp_size = 32;
/*! \brief Maximum number of threads in one block */
static constexpr uint max_block_size = 1024;

/*! \brief Set single device value */
template <typename T>
__global__ static void set_cuda_value(T* const input, const T value)
{
    *input = value;
}

/*! \brief Reduce warp kernel
 *
 * Differently from the reference, we are not reducing in shared memory
 * but via threads register
 */
template <typename T, unsigned int pixel_per_block, typename R_OP>
__device__ static void warp_primitive_reduce(T* sdata, const uint tid, const R_OP reduce_op)
{
    /* A thread warp (32) works on twice its size
     * __shfl_down_sync instruction use thread register to reduce
     * In our case each thread hold one register
     *
     * This first step needs to be done in shared memory because
     * the pixel located further than warp size can't be stored in register
     *
     * This function can work on less than 64 pixels
     * Because the shared data is initialized to 0 for the missing pixels
     * Executing useless += is more performant than branching with multiple if
     */
    T acc_reg = reduce_op(sdata[tid], sdata[tid + warp_size]);

    if (pixel_per_block >= warp_size)
        acc_reg = reduce_op(acc_reg, __shfl_down_sync(FULL_MASK, acc_reg, warp_size / 2));

    if (pixel_per_block >= warp_size / 2)
        acc_reg = reduce_op(acc_reg, __shfl_down_sync(FULL_MASK, acc_reg, warp_size / 4));

    if (pixel_per_block >= warp_size / 4)
        acc_reg = reduce_op(acc_reg, __shfl_down_sync(FULL_MASK, acc_reg, warp_size / 8));

    if (pixel_per_block >= warp_size / 8)
        acc_reg = reduce_op(acc_reg, __shfl_down_sync(FULL_MASK, acc_reg, warp_size / 16));

    if (pixel_per_block >= warp_size / 16)
        acc_reg = reduce_op(acc_reg, __shfl_down_sync(FULL_MASK, acc_reg, warp_size / 32));

    if (tid == 0)
        sdata[tid] = acc_reg;
}

/*! \brief Reduce kernel
 *
 * The reference code of this kernel can be found here :
 * https://developer.download.nvidia.com/assets/cuda/files/reduction.pdf
 * We highly recommand developers that wants to modify this kernel
 * to first checkout and fully understand the above reference
 *
 * We slightly improved performances by tuning it
 */
template <typename T, typename U, uint pixel_per_block, typename R_OP, typename A_OP>
__global__ static void kernel_reduce(U* const __restrict__ output,
                                     const T* const __restrict__ input,
                                     const uint size,
                                     const R_OP reduce_op,
                                     const A_OP atomic_op,
                                     const T identity_elt)
{
    // Each block is reduced in shared data (avoiding multiple global memory acceses)
    __shared__ T sdata[pixel_per_block / 2];
    const uint tid = threadIdx.x;

    sdata[tid] = identity_elt;
    /* Scope to apply stride loop design pattern
    ** The goal is to have a flexible kernel that can handle input sizes bigger than the grid
    ** This increases threads work and thus improves performances
    */
    {
        const uint problem_stride = pixel_per_block * gridDim.x;

        // Accumulate in shared memory the first reduction step on multiple frame blocks
        for (uint i = blockIdx.x * pixel_per_block + tid;; i += problem_stride)
        {
            if (i + (pixel_per_block / 2) < size)
                sdata[tid] = reduce_op(sdata[tid], reduce_op(input[i], input[i + (pixel_per_block / 2)]));
            else if (i < size)
                sdata[tid] = reduce_op(sdata[tid], input[i]);
            else
                break;
        }
    }

    // Wait for all threads of the block to finish accumulation
    __syncthreads();

    // At this step, the number of pixel per block has been halved by the first reduction step

    // Thread block reduction
    if (pixel_per_block / 2 >= max_block_size)
    {
        if (tid < max_block_size / 2)
            sdata[tid] = reduce_op(sdata[tid], sdata[tid + max_block_size / 2]);
        __syncthreads();
    }
    if (pixel_per_block / 2 >= max_block_size / 2)
    {
        if (tid < max_block_size / 4)
            sdata[tid] = reduce_op(sdata[tid], sdata[tid + max_block_size / 4]);
        __syncthreads();
    }
    if (pixel_per_block / 2 >= max_block_size / 4)
    {
        if (tid < max_block_size / 8)
            sdata[tid] = reduce_op(sdata[tid], sdata[tid + max_block_size / 8]);
        __syncthreads();
    }
    if (pixel_per_block / 2 >= max_block_size / 8)
    {
        if (tid < max_block_size / 16)
            sdata[tid] = reduce_op(sdata[tid], sdata[tid + max_block_size / 16]);
        __syncthreads();
    }

    // The last reductions steps is a warp size problem and can thus be optimized
    if (tid < warp_size)
        warp_primitive_reduce<T, pixel_per_block / 2>(sdata, tid, reduce_op);

    // Each block writes it local reduce to global memory
    if (tid == 0)
        atomic_op(output, sdata[tid]);
}

/*! \brief Reduce operation gpu side
 *
 * \param output Output of the reduce (even with double, imprecision may arise)
 * \param input Input buffer
 * \param size Input size
 * \param reduce_op Operator used for the reduction
 * \param atomic_op Atomic operator used for the write back in output
 * \param identity_elt Identity element needed to initilize data (add needs 0, min needs max...)
 *
 * This kernel has been highly tuned in order to maximize the memory bandwidth usage
 * Numerous benches have been done to achieve the best output possible
 * Don't modify this kernel unless making benches
 */
template <typename T, typename U, typename R_OP, typename A_OP>
void reduce_generic(U* const __restrict__ output,
                    const T* const __restrict__ input,
                    const uint size,
                    const R_OP reduce_op,
                    const A_OP atomic_op,
                    const T identity_elt,
                    const cudaStream_t stream)
{
    // Most optimized grid layout for Holovibes input sizes
    constexpr uint optimal_nb_blocks = 1024;
    constexpr uint optimal_block_size = 128;
    // Handling block_size smaller than 64 would reduce warp_reduce performances
    CHECK(optimal_block_size >= 64);
    // << "kernel reduce only works with with a block size equal or greater than 64 threads (128 pixels)";

    // We still reduce the number of blocks if this reduce is used for really small input
    const uint nb_blocks = std::min((size - 1) / (optimal_block_size * 2) + 1, optimal_nb_blocks);

    /* Reset output to identity element
    ** cudaMemset can only set a value for each byte (can't memset to 0xFFFFFFF because of float)
    ** cudaMemcpyHostToDevice is too slow
    ** The fastest & generic option is using a cuda kernel (~2us)
    */
    set_cuda_value<<<1, 1, 0, stream>>>(output, static_cast<U>(identity_elt));

    // Each thread works at least on 2 pixels
    kernel_reduce<T, U, optimal_block_size * 2>
        <<<nb_blocks, optimal_block_size, 0, stream>>>(output, input, size, reduce_op, atomic_op, identity_elt);
    cudaCheckError();
}

template <typename T, typename U>
void reduce_add(U* const output, const T* const input, const uint size, const cudaStream_t stream)
{
    static const auto op_add = [] __device__(const T left, const T right) { return left + right; };
    static const auto atomic_add = [] __device__(U* const left, const T right) { atomicAdd(left, right); };
    static constexpr T identity_elt_add = static_cast<T>(0);

    reduce_generic(output, input, size, op_add, atomic_add, identity_elt_add, stream);
}

template <typename T>
void reduce_min(T* const output, const T* const input, const uint size, const cudaStream_t stream)
{
    static const auto op_min = [] __device__(const T left, const T right) { return min(left, right); };
    static const auto atomic_min = [] __device__(T* const left, const T right) { atomicMin(left, right); };
    // The identity element of min operation is the max of the type : min(A, max(T)) = A
    static constexpr T identity_elt_min = std::numeric_limits<T>::max();

    reduce_generic(output, input, size, op_min, atomic_min, identity_elt_min, stream);
}

template <typename T>
void reduce_max(T* const output, const T* const input, const uint size, const cudaStream_t stream)
{
    static const auto op_max = [] __device__(const T left, const T right) { return max(left, right); };
    static const auto atomic_max = [] __device__(T* const left, const T right) { atomicMax(left, right); };
    // The identity element of max operation is the lowest of the type : max(A, lowest(T)) = A
    // std::numeric_limits<T>::min must not be used.
    // For floating numbers, min != lowest, lowest = -max.
    // From the cpp reference, the min value of floating numbers is the closest value to 0.
    static constexpr T identity_elt_max = std::numeric_limits<T>::lowest();

    reduce_generic(output, input, size, op_max, atomic_max, identity_elt_max, stream);
}
