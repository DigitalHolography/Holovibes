The input thread and the compute thread are working simultaneously on the input queue. This implies that we need to take care of synchronisations to avoid a data race eventually leading to data corruption. A naive and inefficient approach would be to lock the entire queue using a mutual exclusion (mutex) rule when a thread reads from or writes to the queue. To have more flexibility, we designed a fine-grained method by dividing the input queue into several batches, each of them containing a fixed number of images. A mutex and a dedicated CUDA stream are assigned to each batch slot. CUDA streams allow to perform multiple read and write operations from and to different batches simultaneously on the GPU. Mutexes prevent the consumer thread to read data from a batch while the producer is writing on it and vice versa. This method reduces the number of GPU synchronisations to the minimum because each CUDA stream will handle its memory transfers sequentially without requiring explicit synchronisations in the source code. Finally, our approach eliminates the issue of conflicting memory accesses and increases greatly the input throughput.