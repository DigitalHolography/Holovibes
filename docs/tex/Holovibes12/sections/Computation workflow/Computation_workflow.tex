\input{figures/Holo_article}

In this report, we choose to focus on the implementation details rather than diving into the details of hologram rendering. The basic computation workflow is depicted in Figure \ref{fig:computation_workflow}. Our data structures were created to meet two key requirements: (1) avoid any frame drop in the continuous stream of input images ; (2) enable more efficient parallelisation of the computations used for image rendering. To this effect, we make extensive use of buffers (fixed size contiguous memory area) and queues (ring buffer: fixed-size buffer connected end-to-end) that are allocated in the GPU memory. These structures are designed to be manipulated in a thread-safe fashion without causing a race condition (when the program's behaviour is determined by uncontrollable events such as thread execution order).

\subsection{Input}
\input{sections/Computation workflow/Input}

\subsection{Space and time transformations}
\input{sections/Computation workflow/Space_and_time_transformations}

\subsection{Post-processing and output}
\input{sections/Computation workflow/Post-processing_and_output}

\subsection{Loading and processing input images as batches}
\input{sections/Computation workflow/Loading_and_processing_input_images_as_batches}

\subsection{Leveraging CUDA streams}
\input{sections/Computation workflow/Leveraging_CUDA_streams}

\subsection{Avoiding interruptions of high-speed cameras}
\input{sections/Computation workflow/Avoiding_interruptions_of_high-speed_cameras}

\subsection{Limiting CPU-GPU synchronisations}
\input{sections/Computation workflow/Limiting_CPU-GPU_synchronisations}

\subsection{Handling non-square input frames}
\input{sections/Computation workflow/Handling_non-square_input_frames}

\subsection{Minimising data allocations and transfers}
\input{sections/Computation workflow/Minimising_data_allocations_and_transfers}

\subsection{Micro-cache}
The micro-cache feature represents a memory access optimisation bridging the two key components, the CPU and GPU. It streamlines (and optimises) the transmission of environment variables \textbf{(yet to be defined)} from the State \textbf{(yet to be defined)} to the Back end \textbf{(yet to be defined)}. Leveraging micro-cache yields a substantial boost in overall system performance by diminishing access times to shared data residing between the State and Back end. Without mu-cache, frequent intempestive data transfers between the pinned memory has to...\\

the static micro-cache is a low latency solution that circumvents the issue of the necessity of constant updates between the compute thread and the global state holder.\\

While Holovibes is in the process of computation, you have the capability to modify certain computation settings, such as SpaceTransformation functions or the z-distance parameter, among others. However, a significant challenge arises when attempting to change these variables, as they can only be altered once Holovibes completes all ongoing computations. Some variables, like the z-distance, are relatively straightforward to modify and can be changed almost anytime. On the other hand, variables like Batch-Size require more intricate operations, involving memory de-allocation and buffer reallocation. These operations cannot be performed until we have finished using the data stored in those buffers.

To address this challenge, we developed Micro-cache, which allows us to define "callback-like functions" for each type of variable. With this system, we can specify that the z-distance can be changed immediately, whereas for variables like batch size, which necessitate adjustments to buffer sizes, Micro-cache is designed to wait until all currently computed images have finished processing before applying the changes.

It's important to note that we are discussing the Micro-cache's pipeline here, distinct from the compute pipeline within Holovibes. The compute pipeline is essentially a list of functions that are applied to each batch of frames (with a size of Batch-Size).

For each variable, we determine the necessary actions by traversing a pipeline, which is described in detail below.

\subsection{Time Complexity}
